{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "import math\n",
    "ss = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.sql(\"DROP TABLE IF EXISTS test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisSchema = StructType([StructField(\"sepal_length\", DoubleType(), True), \n",
    "                         StructField(\"sepal_width\", DoubleType(), True),\n",
    "                         StructField(\"petal_length\", DoubleType(), True), \n",
    "                         StructField(\"petal_width\", DoubleType(), True),\n",
    "                         StructField(\"class\", StringType(), True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = ss.read.csv('../Data/iris.csv', schema = irisSchema, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = iris.select('sepal_width','petal_width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|sepal_width|petal_width|\n",
      "+-----------+-----------+\n",
      "|        3.5|        0.2|\n",
      "|        3.0|        0.2|\n",
      "|        3.2|        0.2|\n",
      "|        3.1|        0.2|\n",
      "|        3.6|        0.2|\n",
      "+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sepal_width: double, petal_width: double]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = iris.randomSplit([0.9, 0.1])\n",
    "train.cache()\n",
    "# test.write.saveAsTable(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate covariance.¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.11103140830800402"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covariance = train.cov('sepal_width', 'petal_width')\n",
    "covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = train.select(variance(\"sepal_width\")).first()[0]\n",
    "# variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19166058763931168"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate coefficients and apply the equation\n",
    "# where petal_width = coeff_0 * sepal_width + coeff_1\n",
    "# coeff_0 = covariance(x,y)/variance(x)\n",
    "# coeff_1 = mean(y) – coeff_0 * mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_0 = covariance/variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_1 =  train.select(mean(\"petal_width\")).first()[0] - coeff_0 * train.select(mean(\"sepal_width\")).first()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_output = ss.sql(\"SELECT sepal_width, petal_width, sepal_width * {0} + {1} AS prediction FROM test\".format(coeff_0, coeff_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "sc = ss.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toDoubleSafe(v):\n",
    "    try:\n",
    "        return float(v)\n",
    "    except ValueError:\n",
    "        return str(v) #if it is not a float type return as a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creat an RDD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and convert the data\n",
    "# create census RDD \n",
    "census_raw = sc.textFile(\"../Data/adult.raw\", 4).map(lambda x:  x.split(\",\"))\n",
    "# convert the data to float or string \n",
    "census_raw = census_raw.map(lambda row:  [toDoubleSafe(x) for x in row])\n",
    "# census_raw.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "# create adult schema \n",
    "\n",
    "adultschema = StructType([\n",
    "    StructField(\"age\",DoubleType(),True),\n",
    "    StructField(\"workclass\",StringType(),True),\n",
    "    StructField(\"fnlwgt\",DoubleType(),True),\n",
    "    StructField(\"education\",StringType(),True),\n",
    "    StructField(\"marital_status\",StringType(),True),\n",
    "    StructField(\"occupation\",StringType(),True),\n",
    "    StructField(\"relationship\",StringType(),True),\n",
    "    StructField(\"race\",StringType(),True),\n",
    "    StructField(\"sex\",StringType(),True),\n",
    "    StructField(\"capital_gain\",DoubleType(),True),\n",
    "    StructField(\"capital_loss\",DoubleType(),True),\n",
    "    StructField(\"hours_per_week\",DoubleType(),True),\n",
    "    StructField(\"native_country\",StringType(),True),\n",
    "    StructField(\"income\",StringType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfraw = ss.createDataFrame(census_raw, adultschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+--------+----------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
      "| age|        workclass|  fnlwgt| education|     marital_status|        occupation|  relationship|  race|    sex|capital_gain|capital_loss|hours_per_week|native_country|income|\n",
      "+----+-----------------+--------+----------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
      "|39.0|        State-gov| 77516.0| Bachelors|      Never-married|      Adm-clerical| Not-in-family| White|   Male|      2174.0|         0.0|          40.0| United-States| <=50K|\n",
      "|50.0| Self-emp-not-inc| 83311.0| Bachelors| Married-civ-spouse|   Exec-managerial|       Husband| White|   Male|         0.0|         0.0|          13.0| United-States| <=50K|\n",
      "|38.0|          Private|215646.0|   HS-grad|           Divorced| Handlers-cleaners| Not-in-family| White|   Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|53.0|          Private|234721.0|      11th| Married-civ-spouse| Handlers-cleaners|       Husband| Black|   Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|28.0|          Private|338409.0| Bachelors| Married-civ-spouse|    Prof-specialty|          Wife| Black| Female|         0.0|         0.0|          40.0|          Cuba| <=50K|\n",
      "+----+-----------------+--------+----------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfraw.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the count values of the columns with missing values to get the most common ones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|        workclass|count|\n",
      "+-----------------+-----+\n",
      "|          Private|33906|\n",
      "| Self-emp-not-inc| 3862|\n",
      "|        Local-gov| 3136|\n",
      "|                ?| 2799|\n",
      "|        State-gov| 1981|\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------+-----+\n",
      "|      occupation|count|\n",
      "+----------------+-----+\n",
      "|  Prof-specialty| 6172|\n",
      "|    Craft-repair| 6112|\n",
      "| Exec-managerial| 6086|\n",
      "|    Adm-clerical| 5611|\n",
      "|           Sales| 5504|\n",
      "+----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------+-----+\n",
      "|native_country|count|\n",
      "+--------------+-----+\n",
      "| United-States|43832|\n",
      "|        Mexico|  951|\n",
      "|             ?|  857|\n",
      "|   Philippines|  295|\n",
      "|       Germany|  206|\n",
      "+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfraw.groupBy('workclass').count().orderBy('count', ascending= False).show(5)\n",
    "dfraw.groupBy(dfraw[\"occupation\"]).count().orderBy(\"count\",ascending=False).show(5)\n",
    "dfraw.groupBy(dfraw[\"native_country\"]).count().orderBy(\"count\",ascending=False).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data imputaion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  replace the ? --> dfraw.replace('value_toreplace','replacing value', 'column_name' )\n",
    "dfraw_1 = dfraw.replace('?', 'Private', 'workclass')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+---------+--------------+----------+------------+----+---+------------+------------+--------------+--------------+------+\n",
      "|age|workclass|fnlwgt|education|marital_status|occupation|relationship|race|sex|capital_gain|capital_loss|hours_per_week|native_country|income|\n",
      "+---+---------+------+---------+--------------+----------+------------+----+---+------------+------------+--------------+--------------+------+\n",
      "+---+---------+------+---------+--------------+----------+------------+----+---+------------+------------+--------------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check if ? exists or aleady replaced \n",
    "dfraw_1.filter('workclass == \"?\"').show() # it is empty so it is replaced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  replace the ? --> dfraw.replace('value_toreplace','replacing value', 'column_name' ) in chain \n",
    "dfrawnona = dfraw_1.replace('?', 'Prof-specialty', 'occupation').replace('?', 'United-States', 'native_country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+---------+--------------+----------+------------+----+---+------------+------------+--------------+--------------+------+\n",
      "|age|workclass|fnlwgt|education|marital_status|occupation|relationship|race|sex|capital_gain|capital_loss|hours_per_week|native_country|income|\n",
      "+---+---------+------+---------+--------------+----------+------------+----+---+------------+------------+--------------+--------------+------+\n",
      "+---+---------+------+---------+--------------+----------+------------+----+---+------------+------------+--------------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check if it exists \n",
    "dfrawnona.filter('occupation == \"?\"').show(5) #replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+--------+----------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
      "| age|        workclass|  fnlwgt| education|     marital_status|        occupation|  relationship|  race|    sex|capital_gain|capital_loss|hours_per_week|native_country|income|\n",
      "+----+-----------------+--------+----------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
      "|39.0|        State-gov| 77516.0| Bachelors|      Never-married|      Adm-clerical| Not-in-family| White|   Male|      2174.0|         0.0|          40.0| United-States| <=50K|\n",
      "|50.0| Self-emp-not-inc| 83311.0| Bachelors| Married-civ-spouse|   Exec-managerial|       Husband| White|   Male|         0.0|         0.0|          13.0| United-States| <=50K|\n",
      "|38.0|          Private|215646.0|   HS-grad|           Divorced| Handlers-cleaners| Not-in-family| White|   Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|53.0|          Private|234721.0|      11th| Married-civ-spouse| Handlers-cleaners|       Husband| Black|   Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|28.0|          Private|338409.0| Bachelors| Married-civ-spouse|    Prof-specialty|          Wife| Black| Female|         0.0|         0.0|          40.0|          Cuba| <=50K|\n",
      "+----+-----------------+--------+----------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfrawnona.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert string to numeric values \n",
    "## Logistic reg takes numeric values for modeling so we have to convert the strings to ints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: double (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: double (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- marital_status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: double (nullable = true)\n",
      " |-- capital_loss: double (nullable = true)\n",
      " |-- hours_per_week: double (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the string values \n",
    "dfrawnona.printSchema() # We have education, workclass, occupation, relationship, race, sex, native_country, income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert strings to categorical numeric values\n",
    "from pyspark.ml.feature import StringIndexer # import StringIndexer\n",
    "\n",
    "def indexStringColumn(df, cols): \n",
    "    newdf = df #  assign original df to a newdf \n",
    "#     loop through the columns \n",
    "    for s in cols: \n",
    "#         replace the column \n",
    "        si = StringIndexer(inputCol=s, outputCol=s+'-num' )\n",
    "#     si is estimator, fit the model \n",
    "        sm = si.fit(newdf)\n",
    "#         transform the data set \n",
    "        newdf = sm.transform(newdf).drop(s)\n",
    "        newdf = newdf.withColumnRenamed(s+\"-num\", s)\n",
    "    return newdf\n",
    "# pass all the columns that are strings \n",
    "dfnumeric = indexStringColumn(dfrawnona, [\"workclass\", \"education\",\n",
    "                                           \"marital_status\", \"occupation\",\n",
    "                                           \"relationship\", \"race\", \"sex\", \n",
    "                                           \"native_country\", \"income\"])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+------------+------------+--------------+---------+---------+--------------+----------+------------+----+---+--------------+------+\n",
      "| age|  fnlwgt|capital_gain|capital_loss|hours_per_week|workclass|education|marital_status|occupation|relationship|race|sex|native_country|income|\n",
      "+----+--------+------------+------------+--------------+---------+---------+--------------+----------+------------+----+---+--------------+------+\n",
      "|39.0| 77516.0|      2174.0|         0.0|          40.0|      4.0|      2.0|           1.0|       3.0|         1.0| 0.0|0.0|           0.0|   0.0|\n",
      "|50.0| 83311.0|         0.0|         0.0|          13.0|      1.0|      2.0|           0.0|       2.0|         0.0| 0.0|0.0|           0.0|   0.0|\n",
      "|38.0|215646.0|         0.0|         0.0|          40.0|      0.0|      0.0|           2.0|       9.0|         1.0| 0.0|0.0|           0.0|   0.0|\n",
      "|53.0|234721.0|         0.0|         0.0|          40.0|      0.0|      5.0|           0.0|       9.0|         0.0| 1.0|0.0|           0.0|   0.0|\n",
      "|28.0|338409.0|         0.0|         0.0|          40.0|      0.0|      2.0|           0.0|       0.0|         4.0| 1.0|1.0|           9.0|   0.0|\n",
      "+----+--------+------------+------------+--------------+---------+---------+--------------+----------+------------+----+---+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfnumeric.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: double (nullable = true)\n",
      " |-- fnlwgt: double (nullable = true)\n",
      " |-- capital_gain: double (nullable = true)\n",
      " |-- capital_loss: double (nullable = true)\n",
      " |-- hours_per_week: double (nullable = true)\n",
      " |-- workclass: double (nullable = false)\n",
      " |-- education: double (nullable = false)\n",
      " |-- marital_status: double (nullable = false)\n",
      " |-- occupation: double (nullable = false)\n",
      " |-- relationship: double (nullable = false)\n",
      " |-- race: double (nullable = false)\n",
      " |-- sex: double (nullable = false)\n",
      " |-- native_country: double (nullable = false)\n",
      " |-- income: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check if all the values are changed to numeric values \n",
    "dfnumeric.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoder \n",
    "# # Expand the Columns to as many columns as there a distinct values in it and only one column will 1 and others 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "def oneHotEncodeColumns(df, cols):\n",
    "    newdf = df\n",
    "    for c in cols:\n",
    "        #For each given colum, create OneHotEncoder. \n",
    "        #dropLast : Whether to drop the last category in the encoded vector (default: true)\n",
    "        ohe = OneHotEncoder(inputCol=c, outputCol=c+\"-onehot\", dropLast=False)\n",
    "        ohe_model = ohe.fit(newdf)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-onehot\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-onehot\" suffix. \n",
    "        newdf = ohe_model.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-onehot\", c)\n",
    "    return newdf\n",
    "\n",
    "dfhot = oneHotEncodeColumns(dfnumeric, [\"workclass\", \"education\", \n",
    "                                        \"marital_status\", \"occupation\", \n",
    "                                        \"relationship\", \"race\", \"native_country\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------+\n",
      "|     education|native_country|marital_status|\n",
      "+--------------+--------------+--------------+\n",
      "|(16,[2],[1.0])|(42,[0],[1.0])| (7,[1],[1.0])|\n",
      "|(16,[2],[1.0])|(42,[0],[1.0])| (7,[0],[1.0])|\n",
      "|(16,[0],[1.0])|(42,[0],[1.0])| (7,[2],[1.0])|\n",
      "|(16,[5],[1.0])|(42,[0],[1.0])| (7,[0],[1.0])|\n",
      "|(16,[2],[1.0])|(42,[9],[1.0])| (7,[0],[1.0])|\n",
      "+--------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfhot.select('education','native_country','marital_status' ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+------------+------------+--------------+---+------+-------------+--------------+--------------+--------------+-------------+-------------+--------------+\n",
      "| age|  fnlwgt|capital_gain|capital_loss|hours_per_week|sex|income|    workclass|     education|marital_status|    occupation| relationship|         race|native_country|\n",
      "+----+--------+------------+------------+--------------+---+------+-------------+--------------+--------------+--------------+-------------+-------------+--------------+\n",
      "|39.0| 77516.0|      2174.0|         0.0|          40.0|0.0|   0.0|(9,[4],[1.0])|(16,[2],[1.0])| (7,[1],[1.0])|(15,[3],[1.0])|(6,[1],[1.0])|(5,[0],[1.0])|(42,[0],[1.0])|\n",
      "|50.0| 83311.0|         0.0|         0.0|          13.0|0.0|   0.0|(9,[1],[1.0])|(16,[2],[1.0])| (7,[0],[1.0])|(15,[2],[1.0])|(6,[0],[1.0])|(5,[0],[1.0])|(42,[0],[1.0])|\n",
      "|38.0|215646.0|         0.0|         0.0|          40.0|0.0|   0.0|(9,[0],[1.0])|(16,[0],[1.0])| (7,[2],[1.0])|(15,[9],[1.0])|(6,[1],[1.0])|(5,[0],[1.0])|(42,[0],[1.0])|\n",
      "|53.0|234721.0|         0.0|         0.0|          40.0|0.0|   0.0|(9,[0],[1.0])|(16,[5],[1.0])| (7,[0],[1.0])|(15,[9],[1.0])|(6,[0],[1.0])|(5,[1],[1.0])|(42,[0],[1.0])|\n",
      "|28.0|338409.0|         0.0|         0.0|          40.0|1.0|   0.0|(9,[0],[1.0])|(16,[2],[1.0])| (7,[0],[1.0])|(15,[0],[1.0])|(6,[4],[1.0])|(5,[1],[1.0])|(42,[9],[1.0])|\n",
      "+----+--------+------------+------------+--------------+---+------+-------------+--------------+--------------+--------------+-------------+-------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfhot.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a feature Vector through VectorAssembler\n",
    "\n",
    "Merge all the new vectors and the original columns into a single vector. \n",
    "◦Useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees.  \n",
    "◦ML algorithms work with two columns called features and label by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the data with Vector Assembler.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "input_cols=[\"age\",\"capital_gain\",\"capital_loss\",\"fnlwgt\",\"hours_per_week\",\"sex\",\"workclass\",\n",
    "            \"education\",\"marital_status\",\"occupation\",\"relationship\",\"native_country\",\"race\"]\n",
    "\n",
    "#VectorAssembler takes a number of collumn names(inputCols) and output column name (outputCol)\n",
    "#and transforms a DataFrame to assemble the values in inputCols into one single vector with outputCol.\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=input_cols)\n",
    "#lpoints - labeled data.\n",
    "lpoints = va.transform(dfhot).select(\"features\", \"income\").withColumnRenamed(\"income\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=SparseVector(106, {0: 39.0, 1: 2174.0, 3: 77516.0, 4: 40.0, 10: 1.0, 17: 1.0, 32: 1.0, 41: 1.0, 54: 1.0, 59: 1.0, 101: 1.0}), label=0.0),\n",
       " Row(features=SparseVector(106, {0: 50.0, 3: 83311.0, 4: 13.0, 7: 1.0, 17: 1.0, 31: 1.0, 40: 1.0, 53: 1.0, 59: 1.0, 101: 1.0}), label=0.0),\n",
       " Row(features=SparseVector(106, {0: 38.0, 3: 215646.0, 4: 40.0, 6: 1.0, 15: 1.0, 33: 1.0, 47: 1.0, 54: 1.0, 59: 1.0, 101: 1.0}), label=0.0),\n",
       " Row(features=SparseVector(106, {0: 53.0, 3: 234721.0, 4: 40.0, 6: 1.0, 20: 1.0, 31: 1.0, 47: 1.0, 53: 1.0, 59: 1.0, 102: 1.0}), label=0.0),\n",
       " Row(features=SparseVector(106, {0: 28.0, 3: 338409.0, 4: 40.0, 5: 1.0, 6: 1.0, 17: 1.0, 31: 1.0, 38: 1.0, 57: 1.0, 68: 1.0, 102: 1.0}), label=0.0)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lpoints.rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(106,[0,1,3,4,10,...|  0.0|\n",
      "|(106,[0,3,4,7,17,...|  0.0|\n",
      "|(106,[0,3,4,6,15,...|  0.0|\n",
      "|(106,[0,3,4,6,20,...|  0.0|\n",
      "|(106,[0,3,4,5,6,1...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lpoints.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide the dataset into training and vaildation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48842"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lpoints.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = lpoints.randomSplit([0.8, 0.2])\n",
    "\n",
    "#cache() : the algorithm is interative and training and data sets are going to be reused many times.\n",
    "adulttrain = splits[0].cache()\n",
    "adultvalid = splits[1].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adulttrain.write.saveAsTable(\"adulttrain\")\n",
    "adultvalid.write.saveAsTable(\"adultvalid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model.\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(regParam=0.01, maxIter=1000, fitIntercept=True)\n",
    "lrmodel = lr.fit(adulttrain)\n",
    "\n",
    "#The above lines are same as..\n",
    "#lr = LogisticRegression()\n",
    "#lrmodel = lr.setParams(regParam=0.01, maxIter=1000, fitIntercept=True).fit(adulttrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.402248748487637\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Interpret the model parameters\n",
    "# print(lrmodel.coefficients)\n",
    "print(lrmodel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(106,[0,1,3,4,5,6...|  0.0|[-0.4272191290476...|[0.39479057301260...|       1.0|\n",
      "|(106,[0,1,3,4,5,6...|  0.0|[-0.3200456820689...|[0.42066461482132...|       1.0|\n",
      "|(106,[0,1,3,4,5,6...|  0.0|[0.29589415750928...|[0.57343850402302...|       0.0|\n",
      "|(106,[0,1,3,4,5,6...|  0.0|[3.14568216857700...|[0.95873824969205...|       0.0|\n",
      "|(106,[0,1,3,4,5,6...|  0.0|[2.96539417553716...|[0.95098603757053...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluate models using test dataset.\n",
    "#First, transform the validation set.\n",
    "validpredicts = lrmodel.transform(adultvalid)\n",
    "validpredicts.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Our Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderROC:0.8967703011571972\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "bceval = BinaryClassificationEvaluator()\n",
    "print(bceval.getMetricName() + \":\" + str(bceval.evaluate(validpredicts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
