{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "sc = ss.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[47.0,\n",
       "  100.0,\n",
       "  27.0,\n",
       "  81.0,\n",
       "  57.0,\n",
       "  37.0,\n",
       "  26.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  23.0,\n",
       "  56.0,\n",
       "  53.0,\n",
       "  100.0,\n",
       "  90.0,\n",
       "  40.0,\n",
       "  98.0,\n",
       "  8.0]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the data and create an RDD (16 pixels and label)\n",
    "pen_raw = sc.textFile(\"../Data/penbased.dat\", 4)\\\n",
    "            .map(lambda x:  x.split(\", \"))\\\n",
    "            .map(lambda row: [float(x) for x in row])\n",
    "pen_raw.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "penschema = StructType([\n",
    "    StructField(\"pix1\",DoubleType(),True),\n",
    "    StructField(\"pix2\",DoubleType(),True),\n",
    "    StructField(\"pix3\",DoubleType(),True),\n",
    "    StructField(\"pix4\",DoubleType(),True),\n",
    "    StructField(\"pix5\",DoubleType(),True),\n",
    "    StructField(\"pix6\",DoubleType(),True),\n",
    "    StructField(\"pix7\",DoubleType(),True),\n",
    "    StructField(\"pix8\",DoubleType(),True),\n",
    "    StructField(\"pix9\",DoubleType(),True),\n",
    "    StructField(\"pix10\",DoubleType(),True),\n",
    "    StructField(\"pix11\",DoubleType(),True),\n",
    "    StructField(\"pix12\",DoubleType(),True),\n",
    "    StructField(\"pix13\",DoubleType(),True),\n",
    "    StructField(\"pix14\",DoubleType(),True),\n",
    "    StructField(\"pix15\",DoubleType(),True),\n",
    "    StructField(\"pix16\",DoubleType(),True),\n",
    "    StructField(\"label\",DoubleType(),True)\n",
    "])\n",
    "\n",
    "dfpen = ss.createDataFrame(pen_raw, penschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+-----+-----+-----+-----+-----+----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|pix1| pix2|pix3| pix4| pix5| pix6| pix7| pix8|pix9|pix10|pix11|pix12|pix13|pix14|pix15|pix16|label|\n",
      "+----+-----+----+-----+-----+-----+-----+-----+----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|47.0|100.0|27.0| 81.0| 57.0| 37.0| 26.0|  0.0| 0.0| 23.0| 56.0| 53.0|100.0| 90.0| 40.0| 98.0|  8.0|\n",
      "| 0.0| 89.0|27.0|100.0| 42.0| 75.0| 29.0| 45.0|15.0| 15.0| 37.0|  0.0| 69.0|  2.0|100.0|  6.0|  2.0|\n",
      "| 0.0| 57.0|31.0| 68.0| 72.0| 90.0|100.0|100.0|76.0| 75.0| 50.0| 51.0| 28.0| 25.0| 16.0|  0.0|  1.0|\n",
      "| 0.0|100.0| 7.0| 92.0|  5.0| 68.0| 19.0| 45.0|86.0| 34.0|100.0| 45.0| 74.0| 23.0| 67.0|  0.0|  4.0|\n",
      "| 0.0| 67.0|49.0| 83.0|100.0|100.0| 81.0| 80.0|60.0| 60.0| 40.0| 40.0| 33.0| 20.0| 47.0|  0.0|  1.0|\n",
      "+----+-----+----+-----+-----+-----+-----+-----+----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfpen.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize and Create a data frame includes \"feature\" and \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "va = VectorAssembler(inputCols=dfpen.columns[:-1], outputCol='features')\n",
    "\n",
    "penlpoints = va.transform(dfpen).select('features', 'label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[47.0,100.0,27.0,...|  8.0|\n",
      "|[0.0,89.0,27.0,10...|  2.0|\n",
      "|[0.0,57.0,31.0,68...|  1.0|\n",
      "|[0.0,100.0,7.0,92...|  4.0|\n",
      "|[0.0,67.0,49.0,83...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "penlpoints.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training and Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendtset = penlpoints.randomSplit([0.8, 0.2], 1)\n",
    "pentrain = pendtset[0].cache()\n",
    "pendvalid = pendtset[1].cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Decision tree model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(maxDepth=20, maxBins=32, minInfoGain=0, minInstancesPerNode=1)\n",
    "dtmodel = dt.fit(pentrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dtreeviz\n",
    "# dtmodel.dtreeviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtpredicts = dtmodel.transform(pendvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[0.0,0.0,41.0,16....|  9.0|[0.0,0.0,0.0,0.0,...|[0.0,0.0,0.0,0.0,...|       9.0|\n",
      "|[0.0,20.0,47.0,42...|  1.0|[0.0,378.0,0.0,0....|[0.0,1.0,0.0,0.0,...|       1.0|\n",
      "|[0.0,22.0,36.0,47...|  1.0|[0.0,378.0,0.0,0....|[0.0,1.0,0.0,0.0,...|       1.0|\n",
      "|[0.0,23.0,63.0,46...|  8.0|[0.0,0.0,0.0,0.0,...|[0.0,0.0,0.0,0.0,...|       8.0|\n",
      "|[0.0,26.0,57.0,56...|  8.0|[0.0,0.0,0.0,0.0,...|[0.0,0.0,0.0,0.0,...|       8.0|\n",
      "|[0.0,39.0,2.0,62....|  0.0|[1.0,0.0,0.0,0.0,...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|[0.0,39.0,42.0,52...|  1.0|[0.0,378.0,0.0,0....|[0.0,1.0,0.0,0.0,...|       1.0|\n",
      "|[0.0,40.0,29.0,56...|  1.0|[0.0,378.0,0.0,0....|[0.0,1.0,0.0,0.0,...|       1.0|\n",
      "|[0.0,43.0,26.0,65...|  1.0|[0.0,378.0,0.0,0....|[0.0,1.0,0.0,0.0,...|       1.0|\n",
      "|[0.0,43.0,35.0,60...|  1.0|[0.0,378.0,0.0,0....|[0.0,1.0,0.0,0.0,...|       1.0|\n",
      "|[0.0,45.0,27.0,61...|  1.0|[0.0,378.0,0.0,0....|[0.0,1.0,0.0,0.0,...|       1.0|\n",
      "|[0.0,46.0,52.0,72...|  1.0|[0.0,378.0,0.0,0....|[0.0,1.0,0.0,0.0,...|       1.0|\n",
      "|[0.0,48.0,13.0,65...|  1.0|[0.0,13.0,0.0,0.0...|[0.0,1.0,0.0,0.0,...|       1.0|\n",
      "|[0.0,51.0,19.0,63...|  1.0|[0.0,378.0,0.0,0....|[0.0,1.0,0.0,0.0,...|       1.0|\n",
      "|[0.0,53.0,36.0,77...|  1.0|[0.0,45.0,0.0,0.0...|[0.0,1.0,0.0,0.0,...|       1.0|\n",
      "|[0.0,53.0,45.0,75...|  1.0|[0.0,378.0,0.0,0....|[0.0,1.0,0.0,0.0,...|       1.0|\n",
      "|[0.0,55.0,35.0,84...|  1.0|[0.0,112.0,0.0,0....|[0.0,1.0,0.0,0.0,...|       1.0|\n",
      "|[0.0,55.0,37.0,83...|  1.0|[0.0,112.0,0.0,0....|[0.0,1.0,0.0,0.0,...|       1.0|\n",
      "|[0.0,55.0,53.0,77...|  1.0|[0.0,378.0,0.0,0....|[0.0,1.0,0.0,0.0,...|       1.0|\n",
      "|[0.0,56.0,34.0,77...|  1.0|[0.0,378.0,0.0,0....|[0.0,1.0,0.0,0.0,...|       1.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtpredicts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalaute the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "metric = 'f1'\n",
    "metrics = MulticlassClassificationEvaluator().setLabelCol('label').setPredictionCol('prediction').setMetricName('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9585890253909967"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.evaluate(dtpredicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder \n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator().setLabelCol('label').setPredictionCol('prediction')\n",
    "dt = DecisionTreeClassifier()\n",
    "paramGrid = ParamGridBuilder().addGrid(dt.maxDepth, [5,10,15,20,25,30]).build()\n",
    "\n",
    "cv = CrossValidator(estimator=dt,\n",
    "                   evaluator=evaluator,\n",
    "                   numFolds=5,\n",
    "                   estimatorParamMaps=paramGrid)\n",
    "\n",
    "cvmodel = cv.fit(pentrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Max Depth : <bound method _DecisionTreeParams.getMaxDepth of DecisionTreeClassificationModel: uid=DecisionTreeClassifier_d37adc370e0c, depth=15, numNodes=569, numClasses=10, numFeatures=16>\n",
      "Accuracy : 0.9585890253909967\n"
     ]
    }
   ],
   "source": [
    "dtpredict = cvmodel.bestModel.transform(pendvalid)\n",
    "\n",
    "print(\"Best Max Depth : %s\" % cvmodel.bestModel.getMaxDepth)\n",
    "print(\"Accuracy : %s\" % evaluator.evaluate(dtpredicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "prediciton_label = dtpredicts.select('prediction', 'label').rdd\n",
    "\n",
    "metrics = MulticlassMetrics(prediciton_label)\n",
    "\n",
    "confusionMetrics = metrics.confusionMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[223.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   4.,   0.],\n",
      "             [  0., 197.,  12.,   3.,   0.,   0.,   2.,   0.,   0.,   0.],\n",
      "             [  0.,   4., 200.,   0.,   1.,   0.,   1.,   1.,   0.,   0.],\n",
      "             [  1.,   0.,   1., 185.,   0.,   0.,   0.,   1.,   0.,   3.],\n",
      "             [  0.,   1.,   0.,   0., 216.,   0.,   1.,   0.,   0.,   2.],\n",
      "             [  0.,   0.,   0.,   3.,   1., 176.,   2.,   2.,   2.,   9.],\n",
      "             [  0.,   1.,   0.,   0.,   3.,   1., 192.,   1.,   0.,   0.],\n",
      "             [  0.,   1.,   3.,   1.,   0.,   2.,   0., 189.,   0.,   0.],\n",
      "             [  4.,   0.,   0.,   0.,   0.,   0.,   0.,   1., 169.,   0.],\n",
      "             [  0.,   0.,   0.,   3.,   0.,   3.,   0.,   1.,   0., 178.]])\n"
     ]
    }
   ],
   "source": [
    "print(confusionMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
